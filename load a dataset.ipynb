{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.applications import VGG16\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# VGG16 was designed to work on 224 x 224 pixel input images sizes\n",
    "img_rows = 224\n",
    "img_cols = 224 \n",
    "\n",
    "#include_top=False removes the output layer of the model\n",
    "base_model = VGG16(weights = 'imagenet', \n",
    "                 include_top = False, \n",
    "                 input_shape = (img_rows, img_cols, 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#To get only the name\n",
    "base_model.layers[0].__class__.__name__\n",
    "\n",
    "#To see the input/output of a particular layer\n",
    "base_model.layers[0].input\n",
    "\n",
    "#Freezing all the layers by making their trainable=False\n",
    "for layer in base_model.layers:\n",
    "    layer.trainable=False\n",
    "    \n",
    "#Checking this\n",
    "base_model.layers[10].trainable\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 InputLayer False\n",
      "1 Conv2D False\n",
      "2 Conv2D False\n",
      "3 MaxPooling2D False\n",
      "4 Conv2D False\n",
      "5 Conv2D False\n",
      "6 MaxPooling2D False\n",
      "7 Conv2D False\n",
      "8 Conv2D False\n",
      "9 Conv2D False\n",
      "10 MaxPooling2D False\n",
      "11 Conv2D False\n",
      "12 Conv2D False\n",
      "13 Conv2D False\n",
      "14 MaxPooling2D False\n",
      "15 Conv2D False\n",
      "16 Conv2D False\n",
      "17 Conv2D False\n",
      "18 MaxPooling2D False\n"
     ]
    }
   ],
   "source": [
    "# Let's print our layers \n",
    "for (i,layer) in enumerate(base_model.layers):\n",
    "    print(str(i) + \" \"+ layer.__class__.__name__, layer.trainable)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'block5_pool/MaxPool:0' shape=(None, 7, 7, 512) dtype=float32>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Ouput of the current model\n",
    "base_model.output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Now we add our dense layers for a new prediction on top of the base model\n",
    "from keras.layers import Dense, Flatten\n",
    "from keras.models import Sequential\n",
    "\n",
    "top_model = base_model.output\n",
    "top_model = Flatten()(top_model)\n",
    "top_model = Dense(512, activation='relu')(top_model)   #First added FCL dense layer\n",
    "top_model = Dense(512, activation='relu')(top_model)    #Second added FCL dense layer\n",
    "top_model = Dense(256, activation='relu')(top_model)    #Third added FCL dense layer\n",
    "top_model = Dense(4, activation='softmax')(top_model)    #Output layer with 2 class labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'dense_4/Softmax:0' shape=(None, 4) dtype=float32>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Now let's see the top_model output\n",
    "top_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'input_1:0' shape=(None, 224, 224, 3) dtype=float32>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "base_model.input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#IMP: Mounting the base_model with the top_model and forming newmodel\n",
    "\n",
    "from keras.models import Model\n",
    "newmodel = Model(inputs=base_model.input, outputs=top_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'dense_4/Softmax:0' shape=(None, 4) dtype=float32>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "newmodel.output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<keras.engine.input_layer.InputLayer at 0x22232c38308>,\n",
       " <keras.layers.convolutional.Conv2D at 0x22232c38bc8>,\n",
       " <keras.layers.convolutional.Conv2D at 0x22232c38c88>,\n",
       " <keras.layers.pooling.MaxPooling2D at 0x22232c9ea08>,\n",
       " <keras.layers.convolutional.Conv2D at 0x22232c9edc8>,\n",
       " <keras.layers.convolutional.Conv2D at 0x22232cb1cc8>,\n",
       " <keras.layers.pooling.MaxPooling2D at 0x22232cb4bc8>,\n",
       " <keras.layers.convolutional.Conv2D at 0x22232cb47c8>,\n",
       " <keras.layers.convolutional.Conv2D at 0x22232cbd648>,\n",
       " <keras.layers.convolutional.Conv2D at 0x22232cc2648>,\n",
       " <keras.layers.pooling.MaxPooling2D at 0x22232cd1948>,\n",
       " <keras.layers.convolutional.Conv2D at 0x22232cd1708>,\n",
       " <keras.layers.convolutional.Conv2D at 0x22232cd9e88>,\n",
       " <keras.layers.convolutional.Conv2D at 0x22232ce0c08>,\n",
       " <keras.layers.pooling.MaxPooling2D at 0x22232ce4d48>,\n",
       " <keras.layers.convolutional.Conv2D at 0x22232ce9848>,\n",
       " <keras.layers.convolutional.Conv2D at 0x22232cedd48>,\n",
       " <keras.layers.convolutional.Conv2D at 0x22232cf8dc8>,\n",
       " <keras.layers.pooling.MaxPooling2D at 0x22232cfda88>,\n",
       " <keras.layers.core.Flatten at 0x22232c1fa08>,\n",
       " <keras.layers.core.Dense at 0x22232c32a48>,\n",
       " <keras.layers.core.Dense at 0x22232c0e9c8>,\n",
       " <keras.layers.core.Dense at 0x22232e7b708>,\n",
       " <keras.layers.core.Dense at 0x22232e8a6c8>]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "newmodel.layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         (None, 224, 224, 3)       0         \n",
      "_________________________________________________________________\n",
      "block1_conv1 (Conv2D)        (None, 224, 224, 64)      1792      \n",
      "_________________________________________________________________\n",
      "block1_conv2 (Conv2D)        (None, 224, 224, 64)      36928     \n",
      "_________________________________________________________________\n",
      "block1_pool (MaxPooling2D)   (None, 112, 112, 64)      0         \n",
      "_________________________________________________________________\n",
      "block2_conv1 (Conv2D)        (None, 112, 112, 128)     73856     \n",
      "_________________________________________________________________\n",
      "block2_conv2 (Conv2D)        (None, 112, 112, 128)     147584    \n",
      "_________________________________________________________________\n",
      "block2_pool (MaxPooling2D)   (None, 56, 56, 128)       0         \n",
      "_________________________________________________________________\n",
      "block3_conv1 (Conv2D)        (None, 56, 56, 256)       295168    \n",
      "_________________________________________________________________\n",
      "block3_conv2 (Conv2D)        (None, 56, 56, 256)       590080    \n",
      "_________________________________________________________________\n",
      "block3_conv3 (Conv2D)        (None, 56, 56, 256)       590080    \n",
      "_________________________________________________________________\n",
      "block3_pool (MaxPooling2D)   (None, 28, 28, 256)       0         \n",
      "_________________________________________________________________\n",
      "block4_conv1 (Conv2D)        (None, 28, 28, 512)       1180160   \n",
      "_________________________________________________________________\n",
      "block4_conv2 (Conv2D)        (None, 28, 28, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block4_conv3 (Conv2D)        (None, 28, 28, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block4_pool (MaxPooling2D)   (None, 14, 14, 512)       0         \n",
      "_________________________________________________________________\n",
      "block5_conv1 (Conv2D)        (None, 14, 14, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block5_conv2 (Conv2D)        (None, 14, 14, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block5_conv3 (Conv2D)        (None, 14, 14, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block5_pool (MaxPooling2D)   (None, 7, 7, 512)         0         \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 25088)             0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 512)               12845568  \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 512)               262656    \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 256)               131328    \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 4)                 1028      \n",
      "=================================================================\n",
      "Total params: 27,955,268\n",
      "Trainable params: 13,240,580\n",
      "Non-trainable params: 14,714,688\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "newmodel.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 1400 images belonging to 4 classes.\n",
      "Found 628 images belonging to 4 classes.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'#Converting to 4D\\nfor img_test in test_generator:\\n    img_test=np.expand_dims(img_test, axis=0)'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Importing our images for recognition\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "train_data=\"\\\\Users/DELL/Desktop/Face/train/\"\n",
    "test_data=\"\\\\Users/DELL/Desktop/Face/test/\"\n",
    "\n",
    "#Data image augmentation\n",
    "train_datagen = ImageDataGenerator(\n",
    "      rescale=1./255,\n",
    "      rotation_range=20,\n",
    "      width_shift_range=0.2,\n",
    "      height_shift_range=0.2,\n",
    "      horizontal_flip=True,\n",
    "      fill_mode='nearest')\n",
    " \n",
    "test_datagen = ImageDataGenerator(rescale=1./255)\n",
    "\n",
    "train_generator = train_datagen.flow_from_directory(\n",
    "        train_data,\n",
    "        target_size=(img_rows, img_cols),\n",
    "        class_mode='categorical')\n",
    "\n",
    "'''#Converting to 4D\n",
    "for img_train in train_generator:\n",
    "    img_train=np.expand_dims(img_train, axis=0)'''\n",
    " \n",
    "test_generator = test_datagen.flow_from_directory(\n",
    "        test_data,\n",
    "        target_size=(img_rows, img_cols),\n",
    "        class_mode='categorical',\n",
    "        shuffle=False)\n",
    "\n",
    "'''#Converting to 4D\n",
    "for img_test in test_generator:\n",
    "    img_test=np.expand_dims(img_test, axis=0)'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Now let's compile our model\n",
    "from keras.optimizers import RMSprop\n",
    "newmodel.compile(optimizer = RMSprop(lr=0.0001),\n",
    "                 loss = 'categorical_crossentropy',\n",
    "                 metrics =['accuracy']\n",
    "                )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "100/100 [==============================] - 1328s 13s/step - loss: 0.4352 - accuracy: 0.8354 - val_loss: 0.0021 - val_accuracy: 0.9013\n",
      "Epoch 2/5\n",
      "100/100 [==============================] - 1324s 13s/step - loss: 0.1513 - accuracy: 0.9531 - val_loss: 1.5002e-05 - val_accuracy: 0.9459\n",
      "Epoch 3/5\n",
      "100/100 [==============================] - 1322s 13s/step - loss: 0.0865 - accuracy: 0.9736 - val_loss: 4.0829e-06 - val_accuracy: 0.9395\n",
      "Epoch 4/5\n",
      "100/100 [==============================] - 1323s 13s/step - loss: 0.0584 - accuracy: 0.9796 - val_loss: 8.1658e-07 - val_accuracy: 0.9506\n",
      "Epoch 5/5\n",
      "100/100 [==============================] - 1334s 13s/step - loss: 0.0479 - accuracy: 0.9840 - val_loss: 7.1526e-08 - val_accuracy: 0.9475\n"
     ]
    }
   ],
   "source": [
    "history = newmodel.fit_generator(train_generator, epochs=5,steps_per_epoch=100, validation_data=test_generator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "newmodel.save('FaceRecog_VGG16.h4')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import load_model\n",
    "\n",
    "classifier = load_model('FaceRecog_VGG16.h4')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class - raghav\n",
      "Class - sambhav\n",
      "Class - sambhav\n",
      "Class - sambhav\n",
      "Class - navika\n",
      "Class - navika\n",
      "Class - pavni\n",
      "Class - pavni\n",
      "Class - navika\n",
      "Class - pavni\n"
     ]
    }
   ],
   "source": [
    "#Testing the Model\n",
    "%matplotlib inline\n",
    "#The line above is necesary to show Matplotlib's plots inside a Jupyter Notebook\n",
    "\n",
    "import cv2\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "import os\n",
    "import cv2\n",
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "\n",
    "four_person_dict = {\"[0]\": \"navika\", \n",
    "                      \"[1]\": \"pavni\",\n",
    "                      \"[2]\": \"raghav\",\n",
    "                      \"[3]\": \"sambhav\",\n",
    "                     }\n",
    "\n",
    "four_person_dict_n = {\"navika\": \"navika\", \n",
    "                      \"pavni\": \"pavni\",\n",
    "                      \"raghav\": \"raghav\",\n",
    "                      \"sambhav\": \"sambhav\",\n",
    "                      }\n",
    "\n",
    "def draw_test(name, pred, im):\n",
    "    person =four_person_dict[str(pred)]\n",
    "    BLACK = [0,0,0]\n",
    "    expanded_image = cv2.copyMakeBorder(im, 80, 0, 0, 100 ,cv2.BORDER_CONSTANT,value=BLACK)\n",
    "    cv2.putText(expanded_image, person, (20, 60) , cv2.FONT_HERSHEY_SIMPLEX,1, (0,0,255), 2)\n",
    "    cv2.imshow(name, expanded_image)\n",
    "\n",
    "def getRandomImage(path):\n",
    "    \"\"\"function loads a random images from a random folder in our test path \"\"\"\n",
    "    folders = list(filter(lambda x: os.path.isdir(os.path.join(path, x)), os.listdir(path)))\n",
    "    random_directory = np.random.randint(0,len(folders))\n",
    "    path_class = folders[random_directory]\n",
    "    print(\"Class - \" + four_person_dict_n[str(path_class)])\n",
    "    file_path = path + path_class\n",
    "    file_names = [f for f in listdir(file_path) if isfile(join(file_path, f))]\n",
    "    random_file_index = np.random.randint(0,len(file_names))\n",
    "    image_name = file_names[random_file_index]\n",
    "    return cv2.imread(file_path+\"/\"+image_name)    \n",
    "\n",
    "for i in range(0,10):\n",
    "    input_im = getRandomImage(\"\\\\Users/DELL/Desktop/Face/test/\")\n",
    "    input_original = input_im.copy()\n",
    "    input_original = cv2.resize(input_original, None, fx=0.5, fy=0.5, interpolation = cv2.INTER_LINEAR)\n",
    "    \n",
    "    input_im = cv2.resize(input_im, (224, 224), interpolation = cv2.INTER_LINEAR)\n",
    "    input_im = input_im / 255.\n",
    "    input_im = input_im.reshape(1,224,224,3) \n",
    "    \n",
    "    # Get Prediction\n",
    "    res = np.argmax(classifier.predict(input_im, 1, verbose = 0), axis=1)\n",
    "    \n",
    "    # Show image with predicted class\n",
    "    draw_test(\"Prediction\", res, input_original) \n",
    "    cv2.waitKey(0)\n",
    "\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
